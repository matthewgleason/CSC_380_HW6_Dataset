{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FNG8Q4tRxOU"
   },
   "source": [
    "# CSC380 - Homework 6\n",
    "This homework will familiarize you with linear regression.  You will be using the *Prostate Cancer Dataset* from a study by Stamey et al. (1989).  The study aims to predict prostate-specific antigen levels from clinical measures in men about to receive a radical prostatectomy.  \n",
    "\n",
    "The data contain 8 features:\n",
    "* log cancer volume (lcavol)\n",
    "* log prostate weight (lweight)\n",
    "* age (age)\n",
    "* log amount of benign prostatic hyperplasia (lbph)\n",
    "* seminal vesicle invasion (svi)\n",
    "* log of capsular penetration (lcp)\n",
    "* Gleason score (gleason)\n",
    "* percent of Gleason scores 4 or 5 (pgg45)\n",
    "\n",
    "The data use a fixed Train / Test split, which we will load below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMMHdFuCM2TB"
   },
   "outputs": [],
   "source": [
    "#All finalised needed imports\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jV0v7VU1O2NS",
    "outputId": "df993db4-f613-4c67-b64c-ab6ef078fb0f"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('prostate_train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A73_lt-e6itZ"
   },
   "source": [
    "## Problem 1: Your First Regression \n",
    "We will begin by fitting our first ordinary least squares regression model.  But first we need to do a little data management.  You will notice that the data exist in a single data frame (one for Train and one for Test).  The last column of the data frame ('lpsa') is the quantity that we wish to predict (the Y-value).  \n",
    "\n",
    "### (a) \n",
    "\n",
    "Do the following in the cell below,\n",
    "* Create X_train and Y_train by separating out the last column ('lpsa') and store it in Y_train\n",
    "* Do the same for X_test and Y_test\n",
    "* Display the DataFrame X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4rJP2qpRSqXm",
    "outputId": "4bdad063-2e64-4fac-d37d-cd1ab2959108"
   },
   "outputs": [],
   "source": [
    "features = ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\n",
    "output = ['lpsa']\n",
    "X_train = # Insert code here\n",
    "Y_train = # Insert code here\n",
    "\n",
    "# Display training inputs\n",
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ETPysbP8Fc6"
   },
   "source": [
    "### (b)\n",
    "\n",
    "Now we will fit our first model using a single feature ('lcavol').  Do the following in the cell below,\n",
    "* Train a linear regression model on the 'lcavol' feature\n",
    "* Compute the R-squared score of the model on the training data\n",
    "* Scatterplot the training data for the 'lcavol' feature\n",
    "* Plot the regression line over the scatterplot \n",
    "* Label the plot axis / title and report the R-squared score\n",
    "\n",
    "A couple of notes:\n",
    "* Scikit-learn gets cranky when you pass in single features.  In some versions you will need to use, X_train['lcavol'].values.reshape(-1, 1)\n",
    "* To plot the regression line you can create a dense grid of points using numpy.arange, between the min() and max() of the feature values.\n",
    "\n",
    "[Documentation - Scikit-Learn - LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "v8RGYe8xP2iO",
    "outputId": "8f3d4974-13ae-48ac-febe-f7a0292a6b16"
   },
   "outputs": [],
   "source": [
    "# Fit one feature\n",
    "# Insert code here\n",
    "\n",
    "# plot\n",
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAKhTAdp-Lq9"
   },
   "source": [
    "## Problem 2: Best Subset Feature Selection\n",
    "Now we will look at finding the best subset of features out of all possible subsets.  To do this, you will implement the Best Feature Subset Selection as presented in lecture (see lecture slides).  We will break this into subproblems to walk through it.  To help you with this we have provided a function findsubsets(S,k).  When passed a set S this function will return a set of all subsets of size k, which you can iterate through to train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jNdoPr4lYw4"
   },
   "outputs": [],
   "source": [
    "def findsubsets(S,k):\n",
    "    return set(itertools.combinations(S, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Zy6jrtRllHM"
   },
   "source": [
    "### (a)\n",
    "\n",
    "We will start by getting familiar with the findsubsets() function.  The variable 'features' was defined previously as a set of all feature names.  In the cell do the following:\n",
    "* Use findsubsets to find all possible subsets of 3 features\n",
    "* Perform 5-fold cross validation to train a LinearRegression model on each set of 3 features\n",
    "* Find the model with the highest average $R^2$ score (scoring='r2')\n",
    "* Report the best performing set of features and the corresponding $R^2$ score\n",
    "\n",
    "[Documentation - Scikit-Learn - cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmfGnsvgmmKO",
    "outputId": "72faa203-5832-422c-e478-3858439b40c4"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN-qL4N_n0tI"
   },
   "source": [
    "### (b)\n",
    "\n",
    "Now, repeat the above process for all subsets of all sizes.  For each $k=1,\\ldots,8$ find all possible subsets of $k$ features and evaluate a model on each set of features using 5-fold cross validation.  Report your findings as follows,\n",
    "* Produce a scatterplot of $R^2$ values for every run with subset size on the horizontal axis, and $R^2$ on the vertical axis (label your plot axes/title)\n",
    "* Find the best performing model overall and report the $R^2$ and features for that model\n",
    "\n",
    "*Hint: The plot you will produce should look similar to one presented during lecture on Best Subset Selection.* [See lecture slides](http://www.pachecoj.com/courses/csc380_fall21/lectures/linearmodels.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "dWIabcOp884h",
    "outputId": "233cfa21-47e6-4626-d41d-e9beeaaec76b"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJh7JMiiu-Y9"
   },
   "source": [
    "**Excellent**  You have found the best set of features by brute-force search over all possible features.  Good work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paZwCchQ9vAM"
   },
   "source": [
    "## Problem 3 : Ridge Regression \n",
    "\n",
    "### (a)\n",
    "\n",
    "The problem with brute force search over features is that it doesn't scale well.  We can do it for 8 features, but we can't do it for larger sets of features.  Instead, we will look at a simpler model selection strategy by using L2 regularized linear regression (a.k.a. Ridge Regression).  Do the following in the cell below,\n",
    "* Learn a Ridge regression model on training data with alpha=0.5\n",
    "* Report the learned feature weights using the provided printFeatureWeights function\n",
    "\n",
    "[Documentation - Scikit-Learn - linear_model.Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "602shAoTWTU4",
    "outputId": "27600aab-6aba-424d-cc68-2940461f09ce"
   },
   "outputs": [],
   "source": [
    "def printFeatureWeights(f, w):\n",
    "  for idx in range(len(f)):\n",
    "    print('%s : %f' % (f[idx], w[idx]))\n",
    "\n",
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5UO9AYMwlpY"
   },
   "source": [
    "### (b)\n",
    "\n",
    "We chose the regularization coefficient alpha=0.5 somewhat arbitrarily.  We now need to perform model selection in order to learn the best value of alpha.  We will do that by using cross_val_score over a range of values for alpha.  When searching for regularization parameters it is generally good practice to search in log-domain, rather than linear domain.  For example, we will search in the range $[10^{-1}, 10^3]$.  Using Numpy's \"logspace\" function this corresponds to the range $[-1, 3]$ in log-domain.  In the cell below do the following,\n",
    "* Create a range of 50 alpha values spaced logarithmically in the range $[10^{-1}, 10^3]$\n",
    "* Perform 5-fold cross-validation of Ridge regression model for each alpha and record $R^2$ score for each run (there will be 5x50 values)\n",
    "* Report the best $R^2$ score and the value of alpha that achieves that score\n",
    "* Use Matplotlib errorbar() function to plot the average $R^2$ with 1 standard deviation error bars for each of the 50 alpha values\n",
    "\n",
    "[Documentation - Matplotlib - errorbar](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html)\n",
    "\n",
    "[Documentation - Numpy - logspace](https://numpy.org/doc/stable/reference/generated/numpy.logspace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "sB7WSC0EWTXr",
    "outputId": "08560d04-22a0-4042-bab7-b4018caf2b06"
   },
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1ugkjup20ag"
   },
   "source": [
    "Now that we have a good model we will look at what it has learned.  Train the Ridge regression model using the selected alpha from the previous cell.  Report the learned feature weights using the printFeatureWeights() function previously provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQGplVV-XAGV"
   },
   "source": [
    "## Problem 3 : LASSO \n",
    "Ridge regression performs shrinkage of the weights using the L2 norm.  This will drive some weights *close* to zero, but not exactly zero.  The LASSO method replaces the L2 penalty with an L1 penalty.  Due to properties of L1 discussed in lecture, this has the effect of learning exactly zero weights on some features when it is supported by the data.  In this problem we will repeat procedure of learning a Ridge regression model, but we will instead use LASSO.  Let's start by fitting a LASSO model with a fixed alpha value.  \n",
    "\n",
    "### (a)\n",
    "\n",
    "In the cell below do the following,\n",
    "* Fit LASSO with alpha=0.1\n",
    "* Use printFeatureWeights() to report the learned feature weights\n",
    "\n",
    "[Documentation - Scikit-Learn - linear_model.Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w86quobv5V28",
    "outputId": "36bb7888-f051-447e-b18d-2fcf528f70f3"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVB9sLc65fDM"
   },
   "source": [
    "### (b)\n",
    "\n",
    "Now we will find a good value of alpha using cross-validation.  Due to differences in how the LASSO model is optimized, there are dedicated methods for performing cross-validation on LASSO.  Scikit-Learn's LassoLarsCV class performs LASSO-specific cross-validation using an optimized [Least Angle Regression](https://en.wikipedia.org/wiki/Least-angle_regression) (LARS) algorithm.  In the cell below do the following,\n",
    "* Using LassoLarsCV perform 20-fold cross validation to solve all solution paths for Lasso\n",
    "* Plot mean +/- standard error of **mean squared error** versus regularization coefficient $\\alpha$\n",
    "* Title the plot and axes\n",
    "* Report the best alpha value and the corresponding average mean squared error from cross-validation\n",
    "\n",
    "Note: LassoLarsCV returns mean squared error, rather than $R^2$.  It also determines the set of $\\alpha$ values automatically, which are stored in the cv_alphas_ attribute.\n",
    "\n",
    "[Documentation - Scikit-Learn - LassoLarsCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "_GkcB0rKWEyS",
    "outputId": "5b4271f5-ced0-4e65-e776-5dd88575194a"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx6Moj3hdhox"
   },
   "source": [
    "## Problem 4 : Evaluate on Test \n",
    "\n",
    "In this problem we will train all of the best performing models chosen by Best Subsets, Ridge Regression, and LASSO.  We will evaluate and compare these models on the test data.  This dataset uses a standard train / test split so we begin by loading test data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hbrUoMTEQ3v6",
    "outputId": "a31322a5-cad8-4d44-cf71-40da63922e2d"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('prostate_test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Recall that all of the data are stored in a single table, with the final column being the output 'lpsa'.  Before evaluating on test you must first create X_test and Y_test input/outputs where Y_test is the final column of the DataFrame, and X_test contains all other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5RrdBTFEsYS"
   },
   "source": [
    "### (b) Best Subsets  \n",
    "In Problem 2 you found the best subset of features for an ordinary least squares regression model by enumerating all feature subsets.  Using the best selected features train the model below and report mean squared error and $R^2$ on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgzXx_9-skMe",
    "outputId": "912aedf5-bc26-47ef-c839-857ca29c0d4e"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xdtq9WWBGk2p"
   },
   "source": [
    "### (c) Ridge Regression\n",
    "\n",
    "In the cell below, train a Ridge Regression model using the optimal regularization coefficient ($\\alpha$) found in Problem 2.  Report mean squared error and $R^2$ on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9y4YfduGk2p",
    "outputId": "bd804bf4-4f3a-4dc2-816c-3a52642f5d3e"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5OYb_BHF5Mn"
   },
   "source": [
    "### (d) LASSO Regression\n",
    "Now, train and evaluate your final model.  Train a Lasso regression using the optimal $\\alpha$ parameters from Problem 3 and report MSE and $R^2$ on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ms-CvC0F5Mp",
    "outputId": "079b272c-1a25-4f20-e1df-32ea9d683a4e"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzCB3rhdI3iW"
   },
   "source": [
    "### (e) Compare feature weights for each model\n",
    "\n",
    "Now let's compare the feature weight learned by each of the three models.  In the cell below, report the regression weights for each feature under Best Subset, Ridge, and Lasso models evaluated above.  To make the output easier to read, please use a Pandas DataFrame to display the data.  To do this, create a Pandas DataFrame where each column contains regression weights for one of the previous models, and then display that DataFrame in the standard fashion.  You should also provide feature names on each of the rows.\n",
    "\n",
    "[Documentation - Pandas - DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usOZK73IMnuG"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HomeWork7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
